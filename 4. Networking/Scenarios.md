# 4. Networking - Scenarios

![bigstock-Man-connecting-network-cables-32461088](https://github.com/user-attachments/assets/758e3f9e-ef65-4b8e-9b26-b7ecdc2c3ead)


### Skills Utilised

<code>Troubleshooting</code> <code>Networking</code> <code>Real Life Scenarios</code> <code>Buisness Needs</code> <code>SLA and Agreements</code> <code>Network Troubleshooting</code> <code>Communication and Problem Solving</code>

## Overview

While being in my role as an IT/AV Technician - there has been many times in which I have had to deal with some networking real life scenarios - putting what I have learnt in my home labs and certifications in networking into real life scenarios where buisness needs and downtime play an important part in being able to get the issue resolved. I feel like in my portfolio this is something that is really good to have as it showcases my experience in networking and IT being able to communicate with colleagues effectively during a difficult time.

This blog is going to cover these scenarios and my perspective on the events that unfolded from them - these include some simple ones to more complex scenarios that have really pushed my knowledge and abilities to my limits. So hope you enjoy these blogs as much as I did trying to resolve them!

## Blog Entry - 02/03/25

This entry is from the 28th of February - however there has been so much since this incident that now is the only time I can actually sit down and do a write up of what happened.

### Brief Overview of Incident

The incident in this blog was a fault that occured with the Wireless Access Points on the residencies site - it was reported to myself at 8am in the morning that some users were having difficulties connecting to the internet from the WIFI - you could get a connection to the Access Points but not to the wider internet. 

### How It Began

The day started with me arriving into the main building on site at around 8am to start my day as usual with some basic tasks that I needed to do during the day. However as I walked into the building I spoke to the Security team who were on the front desk who stated they had reports from some users saying they couldn't access this internet this morning and could only connect to it in one building. At first when the Security team communicated this to me my first thought was this was an isolated incident as there was only one report of it - so I just asked the Security team to put in a ticket as an incident for it and I would go over an investigate it at some point in the day. 

However, I went into my office and then recieved a phone call from the estates site manager who is based in the other building - in which he said he couldn't get a connection at all and after doing a quick survey in the morning he found that all areas of the building had lost internet connection over WIFI. As soon as I got this phone call I knew this was a larger incident and was going to impact **all** the users who are based at the other building (approximately 100 people). So I asked the Estates Manager to put a ticket in for this so that the other incidents could be collated and made my way over to the other site with the Security Team who luckily drove me over with them (the two sites are a 10 min car journey apart). 

In terms of equipment I always carry some spare Cat 5e cables, my handy NetAlly Link Runner network tester and also my laptop so I can at least do some basic networking commands from my command prompt. As this is a WIFI issue I knew I would only be able to do some basic network troubleshooting on site as the WIFI infastructure is maintained by an external company meaning I could only perform limited troubleshooting to begin with. While travelling over my thoughts were going to be what to expect when I arrived, is this going to be something such as a damaged uplink cable between switches, has any updates or configs been pushed to the AP's or switches recently as the WIFI was working fine overnight, looking for anything out of the normal in terms of LED incidcators or error messages. All of this was just to prepare myself for what I was going to see on site of course - it could literally be anything of course.

I then arrived to site and immediately went to the central comms rack which houses the network switches and fibre uplinks between all the buildings - when looking at the network switches I could see that the ports were behaving as normal all showing a green link and green flashing for activity on the ports - all the fibre uplinks looked fine as these go to a master switch which is housed in the comms rack and this was showing green port links and activity indicators. So initial signs were it is not going to be something to do with the switch - the next step then was to look at the AP's which there are 67 of on site across 4 buildings. Luckily one of them is in the office so I could just get a set of ladders and have a look up at them. 

Now this is where the first indication of something being up was showing. The AP's that are used are the <code>NETGEAR WAX625</code> Access points. These AP's are cloud managed through the Netgear Management portal which because these are from an external company are also managed by said external company - on these AP's there are some indiicator lights which are listed below:

<img width="543" alt="WAX625 Indicators " src="https://github.com/user-attachments/assets/3a27d79e-9ed0-4ca7-8cdc-1ab660213506" />

Now when I looked at the one in the office - I noticed that the cloud indicator was rotating between blue, orange and green. Looking at the documentation this indicated that the access point is acting as a node and the mesh setup is in progress. So my first insinct was to go and check the other AP's in the buildings to see what they were doing - and sure enough they were all displaying the same indicator colors on the cloud indicator. So as the wired network was unaffected I could still contact the supplier via the IP phones in the office - when on the phone to the supplier he advised that he had emails from this morning that the AP's had gone down and highlighted that these messages started from **3am** when a **firmware update had been pushed to the AP's through the NETGEAR management system**. He said he had been investigating since 8am this morning and just wanted to try some basic troubleshooting first just to see if it made a difference or not. Now when I heard that there had been a firmware update pushed out to the AP's in the morning - my first thoughts were could this be the cause - I mean I had never had experiences of a firmware update bugging out a whole network to make it go down - however it isn't uncommon, I mean take the Crowdstike incident where an update caused widespread outages at major airports and more. But for now I just wanted to try the basic troubleshooting first without getting too ahead of myself - but something to keep in mind.

The first basic step was to reboot the switches in each building - as the architecture for the site is one main building that then fibre link between three other buildings and have a dedicated POE switch for the AP's in each building. So I went around each building and rebooted the switches for the AP's just to see what effect it had - after around 5 mins I found that this just resulted in the same indicator appearing on all the access points so that didn't work - back on the phone to the supplier it was.

Next basic troubleshooting step - after speaking to the supplier again we just wanted to check that nothing had been updated on our firewall which could be blocking any traffic going between the switches and the AP's - so afte infomring my colleagues at the main site that they would see the connection between the sites drop for around 2 minutes, I went over to our firewall and just unpluuged the link between the AP switch and the fireguard - this had no effective result as I was still getting the same issue. From my perspective it was just the best to try all the basic troubleshooting before it went any further. By this point the incident ticket that had been raised by the Estates Manager had been upgraded from a Low Impact and Urgency to a High - thus making it a Major Incident and comms being sent out across all the buisnesses sites to inform users. 

Final troubleshooting stewp before going any deeper - rebooting the router just in case there was any faults - this would be unlikely as the wired service was still running but it was best to check it anyways - again informed my colleagues they would lose pings from the site and rebooted the router in the main commms - this didn't have any affect but did keep the wired services online.

I let the supplier know at the other end and it was time to see if he could remote into the switches to see any of the issues - however he reported to me he couldn't get into any of the switches via the Web GUI, he did report he had 2 minutes where he could get very basic access before the connection would drop and he wouldn't be able to access any of the other switches. This was a similar pattern to what was happening with the AP's when rebotting as you would get 2 mins access to the internet before the connection would drop. At this point we both decided because the supplier had access to the necessary troubleshooting tools (i.e admin access to router and switches) that he would need to travel up and come to site so we could investigate together what was going on. As we are on a remote site it would take the supplier 2 hours 30 mins to get to site which would bring the time to around 1pm, we decided though this needed to happen and he started to travel his way up.

As the incident was now at a stage were it was declared a Major Incident - I was part of the major incident team to respond and give communication to senior engineers and heads of IT on what was going on and where I was at within the troubleshooting process. We had service bridge meetings when needed and had one to discuss the situation in which I told them where I was up to with the supplier regarding troubleshooting - they were happy with the progress that was being made and decided the next update should be given an hour after the supplier was on site so it would give a better picture of where we are. This is part of the problem with having equipment from an external company - it makes it that much harder in order to control and if there is any issues you are reliant on them while your users and buisness are without WIFI capabilities. Nonetheless, in the situation I was in I knew I would have to work to my best to try to reduce downtime as much as possible.

### 10:30am onwards ðŸ•¥

From this time onwards I was in a predicement in what I was able to do in terms of the wireless network - but I knew there was other things I could do such as making users aware and stakeholders aware of what was going on at the site - so firstly I co-ordinated with the residencies team on site to get communication out to the residents explainng the situation and trying to give estimated time ranges as to when it could be resolved. The main aim from my point was to at least get some level of service back or full service back which at this point in time I did see as being feasible so long as there wasn't a bigger issue going on. While doing this I also communicated what was going on with wider stakeholders who have an interest in the residencies site also making them aware of the situation and giving time frames.

Of course being able to give timeframes when I was not 100% confident on what the issue was, it was very hard to commmunicate across but I tried to re-assure that the issue would get resolved to some sort of level. I think coming from my background in live events - you have a certain mentality that you keep pushing and pushing until the problem gets solved no matter what time it is (which happened quite a lot) so to me to stay on the job until some level or a full service was resumed was not a problem for me and just came as an instinct.

My final thing to do before trying to think of possible issues happening was to update the service platform (we use ServiceNow) with the latest updates including work notes so if the major incident gets taken down - there is clear notes on the events that took place so they can be mitiagted into the future.

If you are still reading at this point then fair enough because this is basically my whole thought process during the incident. 

Now between the time of putting the work notes up on ServiceNow and the supplier turning up - my thinking was just to inspect the equipment again and look back at the notes already made just to see if there was anything that might spring to mind. And sure enough looking at the notes from before the **firmware update** came back to mind - is it possible that the firmware update would have caused a fault between the switch and the AP themselves - I was thinking to myself surely not but there is always a possibility. Researching the firmware version online there was no reports of any faults with the firmware that Netgear or any other users had reported. Strange. Okay is there a configuraiton issue at play? I would only know now when the supplier got to site. Reboots aren't working so it has got to be something else that is persistent when ever the equipment is turned on. Now I know the PoE switches had recently been upgraded but that was to be able to deliver a higher version of PoE going from PoE+ to PoE++ so that the AP's would be able to work better with a better supply. Something to think about though, for now I can keep looking until the supplier arrives...

### 13:30pm Supplier arrives ðŸ•œ






